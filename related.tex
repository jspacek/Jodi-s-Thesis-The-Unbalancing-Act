%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

\chapter{Related Work} 
\label{sec:related}

% Could also add
%\todo{https://code.briarproject.org/briar/tor-circumvention-analytics/}
%\todo{compare to DDOS in resource distro}
%\todo{Enemy at the Gateways}
%\todo{Proxy recycling (unpublished)}

\section{Resource Scheduling and Allocation}

\textbf{Tor's Bridge Distribution.} Tor bridges are private relays or proxies used for censorship circumvention. Tor bridge IPs and fingerprints are distributed out of band using registered email or through a captcha site on the Tor blog. Tor’s BridgeDB authority distributes up to 3 new bridge IPs and corresponding fingerprints to clients based on a hashring uniform distribution \cite{ling2015tor}. Bridge requests are rate limited every 3 hours by email and once a day via https through a centralized bridge distributor. China has discovered and blocked most of the bridges given through the public distribution channels. Bridge enumeration attacks are possible using bulk emails via HTTPS. Tor’s fingerprint shared secret scheme thwarts active probing but can’t prevent bridge discovery using delayed insider attack\cite{fifield2016censors}. 

\textbf{Domain Fronting.} An elegant way to circumvent censors is through domain fronting, where a user is routed through a legitimate intermediary, such as a content delivery network (CDN)\cite{Fifield2017a}. These intermediaries are used to rendezvous with the Tor network, instead of Tor bridges, and do not have to be kept secret from a censor. Domain fronting is the most reliable way to perform the rendezvous handshake and recently it is the only protocol that works in China\footnote{https://blog.torproject.org/domain-fronting-critical-open-web}. However, we see that major CDNs like Google and Amazon no longer support domain fronting. Microsoft's Azure cloud is a temporary fix as there is no formal agreement between Microsoft and Tor to support domain fronting in the future.

\textbf{Power of D Choices.} The Power of D Choices was analyzed for the generalized form of the coupon collector problem in \cite{xu2011generalized}. This includes the case where a collector wants to collect $m$ out of $n$ total coupons. The collector selects $d$ coupons out of the total collection and chooses the least heavily loaded (or least collected) coupon in each draw. This benefits the collector because duplicate coupons are discarded. They show that the expected number of draws to collect $m$ out of a total of $n$ coupons is $(n \log{n})/d + (n/d )(m − 1) \log{\log{n}} + O (mn)$.

\textbf{Proxisch.} Proxisch \cite{jiang2016proxisch} is an application of a scheduling algorithm for proxy distribution in web crawling applications. Although Proxisch is intended for distributed web crawling, the proxy server selection mechanism work has a similar goal shared by the proxy distribution problem; to reduce the risk of assigning high risk proxies to clients. This work estimates the reliability of proxies to order proxies based on a reliability calculation. They use queuing theory to organize proxies by their respective reliability factors. 

To model the life of a proxy, they use an exponential distribution based on the life of an ideal lamp. They show in their simulations that the actual life span of some proxy servers is close to the exponential distribution. This leads to the calculation of an optimal update period for cycling proxies among processes. They compare their result to a polling scheduling solution that illustrates a higher successful service rates within shorter time periods for their solution. Their resource scheduling algorithm approach is similar to a randomized proxy selection based on attributes of proxies, such as time, reputation, or credits because it orders proxies based on criterion. This is a move away from more complex, monolithic designs favouring proxies that earn standing over time within the system. 

It's not possible to directly translate this solution into the problem addressed in this thesis, since their approach to modeling the lifetime of a proxy server is insufficient for a powerful censor. The types of attackers in Proxisch's model block from the proxy server side; a far weaker adversary than a nation state. However, this work does provide useful hints for how one may approach a lightweight proxy distribution design.\\

\textbf{TorBricks.} The TorBricks \cite{zamani2017torbricks} design focuses on attacks on the proxy distributor that includes a privacy preserving proxy distribution for multiple untrusted distributors. They implement a distributed random generation protocol for groups of proxies that collectively generate uniform random numbers that cannot be biased from uniform random. This work distributes a limited number of proxies to guarantee a maximum number of rounds until all honest users can connect to a proxy server. They use a resource competitive analysis to evaluate the performance of their distribution algorithm when it is under attack. This analysis gives the attackers a number of resources $t$ and evaluates the worst case resource cost of some function of $t$. This allows for an adaptive adjustment for resource cost given the proportion of corruption in the system at any point in time. 

TorBricks addresses malicious attacks from subsets of distributed proxy distribution servers. This differs from my threat model, however, the use of Chernoff bounds in their analysis to estimate load balancing is used in this thesis to bound the attacker between best and worst case distributions. Instead of their classic balls-and-bins process, I model the two-choice randomized analysis from Mitzenmacher detailed below \cite{mitzenmacher1996power}. The resource competitive analysis in TorBricks is a clue for my analysis; however I use competitive results to give a competitive ratio between different attack states.

\section{Trust Systems}

%\todo{HYPHAE: Social Secret Sharing https://patternsinthevoid.net/hyphae/hyphae.pdf}

\textbf{Fighting Censorship with Algorithms.}
Mahdian \cite{mahdian2010fighting} studies proxy distribution as an algorithmic problem and gives bounds on the number of proxies required to provide service to clients, some of whom are adversaries. He includes a theoretical analysis of bounds for the number of proxies needed to survive an insider attack. His theorems use k-union-free families of sets, probabilistic methods, and extremal set theory to give lower and upper bounds. 

Mahdian's scheme creates two sets of users, trusted and suspicious, and distributes keys based on the user's membership in one of these two sets. Users are divided into these sets based on their association with compromised keys; they are moved from the trusted group to the suspicious group. Fresh keys are only handed out to trusted users. This adaptive model bounds the number of keys that an adversary can compromise thus providing guarantees for the user with respect to the expected total number of keys required to give every legitimate user access.

In Mahdian's model, a key represents a servlet or proxy server. His scheme assumes a known number of malicious users and there are no bounds on the number of clients a proxy can serve. While maintenance of keys is usually relatively simple in practice, the logistics of proxy server maintenance is more involved. It is an impractical assumption that keys (representing proxies) can be distributed to an unlimited number of users without significant overhead. 

Mahdian's algorithm distributes an increasing number of keys to users in order to reduce the risk posed by a growing number of adversaries. This does not address the case where adversaries are controlled by the same entity, such as a censor. For example, it does not take into consideration the enumeration attack. This attack is further exacerbated by the reuse of keys, as opposed to the removal of suspect keys. A defense for enumeration attacks is to restrict the number of proxies that are distributed so that the attacker only learns a small subset of proxies. In my model, proxies with more client assignments over time are eventually phased out in the proxy preserving mode as they pose a greater risk of exposure.\\

\textbf{Proximax.} The main goal of the Proximax \cite{mccoy2011proximax} reputation-based system is to maximize the yield of a proxy resource, where yield is the number of user-hours per day before a proxy is blocked, that is calculated as the product of usage and lifetime. Each proxy resource is advertised on multiple channels. This novel use of a channel relies on a fast flux technique that piggybacks on DNS infrastructure. Proximax registers multiple proxies to the same domain name and load balances them based on their current utilization and resource risk parameter. 

The usage and risk of a proxy resource is the sum of the risk of each of the channels where it is distributed. Resource risk is calculated as a maximum likelihood estimate of blocking - it is only an approximation because resources are advertised on multiple channels, and the risk per channel cannot be sampled directly. In other words, when a proxy is blocked, there is no way to detect the specific channel that caused the block and so it is difficult to tease out channel from proxy risk. 

% TODO why couldn't the censor just block the domain name? what about the domain name assignment makes this less likely? do the clients get an IP without the domain name?

It's a fact that all proxies will eventually be discovered in proxy distribution, so Proximax adds a trust scheme to delay censor discovery. Registered users build up their reputation score and invite new users, handled by a registration system. The registration system allocates proxies that have higher risk to lower reputation scores. They widely disseminate the location of low risk proxies in order to maximize their yield. This leads to the potential of rapid enumeration attacks, where the best proxies are enumerated in quick succession. 

Proximax's trust scheme is likely to be thwarted by colluding insider attackers where registered users build up reputation to invite other users, as it does not work well in a delayed blocking attack. Indeed, risk is modeled as a Poisson process that assumes unrealistic censor behaviour. Furthermore, the risk approximation may not be particularly useful because if there is only a single attacker assigned to a proxy, and with the reasonable assumption that all attackers are controlled by the same censor entity, then this proxy has the same likelihood of being blocked as a proxy to which several insider attackers are assigned. \\

\textbf{rBridge.} rBridge \cite{wang2013rbridge} is a trust-based reputation system for a Tor bridge distributor that addresses insider attacks, minimizes user wait time for an available proxy, and preserves privacy of client assignment information. The rBridge distributor computes user reputation based on the uptime of bridges to which a user is assigned. A payment system allows users to buy unblocked bridges to prevent repeated blocks. They show rBridge's user-hours served is at least one order of magnitude more than Proximax and that thirsty hours of users waiting for a proxy is minimized. This is mainly achieved by making sure that the overall rate of new bridges outpaces the rate of proxy blocks, and by reserving half of their bridge resources for future invitations.


A significant contribution of rBridge's design is their privacy preserving scheme using anonymous credentials to build trust, invite users, and obtain signed credentials. Restricting proxy assignments can lead to user fingerprinting as there are unique combinations of proxies tied to a single user. Previous work in pseudonymous credentials and oblivious transfer methods don't work well in proxy systems because an attacker can still infer client assignments based on behaviour after a proxy is blocked. rBridge hides bridge assignment from even the distributor by enabling the proxy assignments to be written and updated by users. They take extra measures to maintain integrity using anonymous credentials, one-time tokens and secrets that cannot be forged owing to zero knowledge proofs and blind signatures. 

In addressing the delayed blocking attack, they note that invitation tickets are randomly distributed over all users, so there is a chance that the corrupt user may not receive a ticket. Since tickets cannot be transferred, it is no more likely that an attacker receives a ticket than an honest user. However, they do not provide analysis given that even just one corrupt user is more than enough to block a proxy, therefore an assignment of a proxy to an attacker is more significant than assignment to an honest user. This means that it will take less than half the users to overtake the system based on a uniform distribution of proxies. \\

% TODO what would a calculation of fingerprinting vs. anonymity look like?

